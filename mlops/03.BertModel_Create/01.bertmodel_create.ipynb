{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TinyBert 모델 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오늘 우리가 한거? 요약?\n",
    "모델관점에서 저희가 오늘 한건 pre-trained 된 huawei-noah/TinyBERT_General_4L_312D을 IMDB dataset으로 fine-tune 하는 과정으로 이해하면 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 데이터로드 \n",
    "import pandas as pd\n",
    "\n",
    "# URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] \n",
    "# certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>\n",
    "# 오류 csv파일 로드에서 오류가 났습니다…\n",
    "# http로 실행\n",
    "\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/laxmimerit/All-CSV-ML-Data-Files-Download/master/IMDB-Dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (3.0.0)\n",
      "Requirement already satisfied: filelock in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets) (0.25.0)\n",
      "Requirement already satisfied: packaging in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.train_test_split(test_size=0.3)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment 의 긍/부정 갯수\n",
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35000/35000 [00:00<00:00, 38734.60 examples/s]\n",
      "Map: 100%|██████████| 15000/15000 [00:00<00:00, 36647.22 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', 'label'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', 'label'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {'negative':0, 'positive':1}\n",
    "\n",
    "dataset = dataset.map(lambda x: {'label':label2id[x['sentiment']]})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': \"A tragically wonderful movie... brings us to a Japan that does not exist anymore. Despite Hollywood's technical expertise, I have yet to see a (hollywood) movie that can match the authenticity of the atmosphere in this small town by the river near the sea... Tom Cruise's The Last Samurai looked liked the last installment of the Lord of The Rings in trying to capture rural Old Japan.<br /><br />If you like serene but intense story lines, this is a must see film. It will be a respite from hollow flashy films much like the last 1000 blockbusters you saw. I think this is one of Kurosawa's better stories.<br /><br />Even if it's a movie about geishas and brothels and the complicated rules that govern life in such settings, it did not turn into a skin flick. The characters are full of depth and act with much intensity.\",\n",
       " 'sentiment': 'positive',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0] # It's crazy => positive? negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (4.44.2)\n",
      "Requirement already satisfied: torch in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from transformers) (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='huawei-noah/TinyBERT_General_4L_312D', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 준비가 끝났습니다. => 모델을 불러와야죠.\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hugginc Face에서 tinybert 검색 -> huawei-noah 모델 선택\n",
    "model = 'huawei-noah/TinyBERT_General_4L_312D'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2651, 2003, 6928, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer(dataset['train'][0]['review'])\n",
    "tokenizer(\"Today is monday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    # padding=True => 텍스트 데이터를 동일한 길이로 패딩 처리\n",
    "    # truncation=True => 맥스 토큰 갯수를 넘어가는 문장은 잘라내겠습니다.\n",
    "    # max_length=300 => 토큰화된 결과의 최대 길이를 300개 (300개 이상의 토큰은 자른다. 300개 이하는 패딩처리)\n",
    "    temp = tokenizer(batch['review'], padding=True, truncation=True, max_length=300)\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35000/35000 [00:07<00:00, 4852.50 examples/s]\n",
      "Map: 100%|██████████| 15000/15000 [00:02<00:00, 5005.55 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize, batched=True, batch_size=None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': \"A tragically wonderful movie... brings us to a Japan that does not exist anymore. Despite Hollywood's technical expertise, I have yet to see a (hollywood) movie that can match the authenticity of the atmosphere in this small town by the river near the sea... Tom Cruise's The Last Samurai looked liked the last installment of the Lord of The Rings in trying to capture rural Old Japan.<br /><br />If you like serene but intense story lines, this is a must see film. It will be a respite from hollow flashy films much like the last 1000 blockbusters you saw. I think this is one of Kurosawa's better stories.<br /><br />Even if it's a movie about geishas and brothels and the complicated rules that govern life in such settings, it did not turn into a skin flick. The characters are full of depth and act with much intensity.\",\n",
       " 'sentiment': 'positive',\n",
       " 'label': 1,\n",
       " 'input_ids': [101,\n",
       "  1037,\n",
       "  13800,\n",
       "  3973,\n",
       "  6919,\n",
       "  3185,\n",
       "  1012,\n",
       "  1012,\n",
       "  1012,\n",
       "  7545,\n",
       "  2149,\n",
       "  2000,\n",
       "  1037,\n",
       "  2900,\n",
       "  2008,\n",
       "  2515,\n",
       "  2025,\n",
       "  4839,\n",
       "  4902,\n",
       "  1012,\n",
       "  2750,\n",
       "  5365,\n",
       "  1005,\n",
       "  1055,\n",
       "  4087,\n",
       "  11532,\n",
       "  1010,\n",
       "  1045,\n",
       "  2031,\n",
       "  2664,\n",
       "  2000,\n",
       "  2156,\n",
       "  1037,\n",
       "  1006,\n",
       "  5365,\n",
       "  1007,\n",
       "  3185,\n",
       "  2008,\n",
       "  2064,\n",
       "  2674,\n",
       "  1996,\n",
       "  21452,\n",
       "  1997,\n",
       "  1996,\n",
       "  7224,\n",
       "  1999,\n",
       "  2023,\n",
       "  2235,\n",
       "  2237,\n",
       "  2011,\n",
       "  1996,\n",
       "  2314,\n",
       "  2379,\n",
       "  1996,\n",
       "  2712,\n",
       "  1012,\n",
       "  1012,\n",
       "  1012,\n",
       "  3419,\n",
       "  8592,\n",
       "  1005,\n",
       "  1055,\n",
       "  1996,\n",
       "  2197,\n",
       "  16352,\n",
       "  2246,\n",
       "  4669,\n",
       "  1996,\n",
       "  2197,\n",
       "  18932,\n",
       "  1997,\n",
       "  1996,\n",
       "  2935,\n",
       "  1997,\n",
       "  1996,\n",
       "  7635,\n",
       "  1999,\n",
       "  2667,\n",
       "  2000,\n",
       "  5425,\n",
       "  3541,\n",
       "  2214,\n",
       "  2900,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2065,\n",
       "  2017,\n",
       "  2066,\n",
       "  25388,\n",
       "  2021,\n",
       "  6387,\n",
       "  2466,\n",
       "  3210,\n",
       "  1010,\n",
       "  2023,\n",
       "  2003,\n",
       "  1037,\n",
       "  2442,\n",
       "  2156,\n",
       "  2143,\n",
       "  1012,\n",
       "  2009,\n",
       "  2097,\n",
       "  2022,\n",
       "  1037,\n",
       "  24501,\n",
       "  23270,\n",
       "  2063,\n",
       "  2013,\n",
       "  8892,\n",
       "  5956,\n",
       "  2100,\n",
       "  3152,\n",
       "  2172,\n",
       "  2066,\n",
       "  1996,\n",
       "  2197,\n",
       "  6694,\n",
       "  27858,\n",
       "  2015,\n",
       "  2017,\n",
       "  2387,\n",
       "  1012,\n",
       "  1045,\n",
       "  2228,\n",
       "  2023,\n",
       "  2003,\n",
       "  2028,\n",
       "  1997,\n",
       "  13970,\n",
       "  7352,\n",
       "  10830,\n",
       "  1005,\n",
       "  1055,\n",
       "  2488,\n",
       "  3441,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2130,\n",
       "  2065,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  1037,\n",
       "  3185,\n",
       "  2055,\n",
       "  16216,\n",
       "  24032,\n",
       "  2015,\n",
       "  1998,\n",
       "  27308,\n",
       "  2015,\n",
       "  1998,\n",
       "  1996,\n",
       "  8552,\n",
       "  3513,\n",
       "  2008,\n",
       "  21208,\n",
       "  2166,\n",
       "  1999,\n",
       "  2107,\n",
       "  10906,\n",
       "  1010,\n",
       "  2009,\n",
       "  2106,\n",
       "  2025,\n",
       "  2735,\n",
       "  2046,\n",
       "  1037,\n",
       "  3096,\n",
       "  17312,\n",
       "  1012,\n",
       "  1996,\n",
       "  3494,\n",
       "  2024,\n",
       "  2440,\n",
       "  1997,\n",
       "  5995,\n",
       "  1998,\n",
       "  2552,\n",
       "  2007,\n",
       "  2172,\n",
       "  8015,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "id2label = {0:'negative', 1:'positive'}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model, #  사전 학습된 모델의 이름\n",
    "    num_labels=len(label2id), # 레이블 갯수\n",
    "    label2id=label2id, # 레이블 문자열 데이터를 숫자로 매핑 딕셔너리\n",
    "    id2label=id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from evaluate) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from evaluate) (0.25.0)\n",
      "Requirement already satisfied: packaging in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.10.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/inseop/Desktop/mlops-project-upstage4/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred): # 평가지표\n",
    "    predctions, labels = eval_pred\n",
    "\n",
    "    predctions = np.argmax(predctions, axis=1)\n",
    "\n",
    "    return accuracy.compute(predictions=predctions, references=labels) # 예측값, 실제 정답 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tf-keras\n",
    "!pip install 'transformers[torch]'\n",
    "!pip install accelerate -U\n",
    "\n",
    "# pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='train_dir', # 학습 결과를 저장할 디렉토리\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32, # 각 디바이스당 학습 배치 크기\n",
    "    per_device_eval_batch_size=32, # 각 디바이스당 평가 배치 크기\n",
    "    eval_strategy='epoch' # 매 에포크마다 평가\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    compute_metrics=compute_metrics, # 평가 지표를 계산하 함수\n",
    "    tokenizer=tokenizer # 텍스트를 토큰으로 변환하는 도구\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "# sentences => positve/negative + score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:35<00:00, 13.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.30040422081947327,\n",
       " 'eval_accuracy': 0.8782666666666666,\n",
       " 'eval_runtime': 35.6592,\n",
       " 'eval_samples_per_second': 420.649,\n",
       " 'eval_steps_per_second': 13.152,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('tinybert-sentiment-analysis') # S3 bucket에 업로드\n",
    "# 모델을 불러와서 API로 Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x16b0be590>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('text-classification', model='tinybert-sentiment-analysis', device=device)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.5736461877822876},\n",
       " {'label': 'negative', 'score': 0.9879584312438965}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['OMG, This movie is crazy.....!!!', 'you never watch this movie. i waste my time.']\n",
    "\n",
    "classifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.8030070066452026}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    \"\"\"I have been using coupang for almost 4 years. With every update, they are adding more and more inconveniences for foreigners. With the last update, you can't select any text in the app to translate or to copy and paste into the search bar for more options during searches. When you contact coupang, the representatives responds with \"We don't provide assistance in English\". Other online platforms are still trying to accommodate foreigners and they go out of their way to assist non koreans.\"\"\"\n",
    "]\n",
    "classifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 준비가 끝났습니다. what's the next step?\n",
    "# (2) 모델을 어딘가에 업로드를 해줘야 합니다. => S3\n",
    "# (3) 모델 인퍼런스를 할 수 있도록 FastAPI로 REST API화해서 결과를 내려줘야 합니다.\n",
    "\n",
    "# MLOPS: Infra + S/W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 # AWS 관련 python 라이브러리 -> s3 버킷 생성/업로드, ec2 생성/삭제 -> bash 명령어로 cli 커맨드 실행하셔도 되요.\n",
    "\n",
    "# AWS => boto3, SageMaker (무료입니다. -> 가입할 때 신용카드를 입력하야지 가입이 가능합니다.)\n",
    "# freetier가 만료되신 분들 => gmail을 하나 더 생성 (gmail 30개) / naver계정 / 0~85원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aws 세팅\n",
    "\n",
    "<!-- (1) IAM 권한이 필요 -->\n",
    "(2) AWS CLI 명령어 설치\n",
    "- mac, windows\n",
    "https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\n",
    "\n",
    "credentials 작업 진행을 위함\n",
    "- IAM으로 이동 => 보안 자격 증명 메뉴 클릭\n",
    "- 액세스 키 생성\n",
    "\n",
    "> aws configure\n",
    "(.venv) ➜  mlops-project > aws configure\n",
    "AWS Access Key ID [****************SK6E]: AKIAWBL2CVEI4XCZAH75\n",
    "AWS Secret Access Key [****************aLxT]: 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\n",
    "Default region name [ap-northeast-2]: ap-northeast-2\n",
    "Default output format [json]: \n",
    "\n",
    "설치가 되신 분들은 아래 명령어를 통해 실행이 잘 되는 지 확인 \n",
    "> aws --version\n",
    "    - aws-cli/2.17.11 Python/3.11.9 Darwin/23.6.0 source/arm64\n",
    "\n",
    "(3) 인증작업을 진행해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) s3 bucket 생성\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def create_s3_bucket(bucket_name):\n",
    "    try:\n",
    "        s3.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={'LocationConstraint':'ap-northeast-2'}\n",
    "        )\n",
    "\n",
    "        print(f\"Success to create s3 bucket Name: {bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success to create s3 bucket Name: inseop-mlops\n"
     ]
    }
   ],
   "source": [
    "create_s3_bucket('inseop-mlops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinybert-sentiment-analysis [] ['model.safetensors', 'tokenizer_config.json', 'special_tokens_map.json', 'config.json', 'tokenizer.json', 'training_args.bin', 'vocab.txt'] \n",
      "\n",
      "tinybert-sentiment-analysis/model.safetensors\n",
      "tinybert-sentiment-analysis/tokenizer_config.json\n",
      "tinybert-sentiment-analysis/special_tokens_map.json\n",
      "tinybert-sentiment-analysis/config.json\n",
      "tinybert-sentiment-analysis/tokenizer.json\n",
      "tinybert-sentiment-analysis/training_args.bin\n",
      "tinybert-sentiment-analysis/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# (2) bucket에 model upload\n",
    "import os\n",
    "\n",
    "for root, dir, files in os.walk('tinybert-sentiment-analysis'):\n",
    "    print(root, dir, files, '\\n')\n",
    "\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(root, file_name)\n",
    "        print(file_path)\n",
    "\n",
    "        bucket_name = 'inseop-mlops'\n",
    "        s3.upload_file(file_path, bucket_name, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
